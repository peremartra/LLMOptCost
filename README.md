# LLMOptCost

# Final Solution. 
I created a model with pruning and a double Knowledge Distillation Process. There is also the possibility to Apply quantization (but not mandatory) to the model, but after training, just to avoid memory problems when loading the Model into the GPU. 

With this solution, I have tried to replicate, within the limits, the work done by Nvidia with Llama models [arXiv:2407.14679 [cs.CL]](https://arxiv.org/abs/2407.14679), where they applied Pruning and Knowledge Distillation to create more efficient yet equally effective Llama models.

## Pruning
The pruning method I used is realy simple: L1 unstructured pruning, where 30% of the weights in each linear layer are set to zero based on their magnitude. This method is highly simple and flexible, allowing it to be applied to any model without the need for complex pre-computations.

In contrast, NVIDIA employs a much more advanced pruning strategy that first calculates the importance of different layers, neurons, and attention heads using a sample dataset. This importance ranking allows them to prune more effectively, maintaining higher accuracy while optimizing the model for structured sparsity, which is particularly efficient on modern GPUs.

While NVIDIA’s method is undoubtedly more optimal for performance and hardware efficiency, especially in large-scale deployments, my approach is simple and reusable, as it can be applied across various models without significant modification.

Notebook: [12_pruning.ipynb](https://github.com/peremartra/LLMOptCost/blob/main/11/12_pruning.ipynb)

Model Created: [bloomz-560m-pruned](https://huggingface.co/oopere/bloomz-560m-pruned)

### 30-Sept Update. 
I’ve added a new notebook on pruning, this time using structured pruning to remove the neurons with the lowest weights from the model.

In this model, the neurons with the lowest L2 norm values are identified, indicating that they may have less influence on the model’s output.

The modified layers are the feed-forward layers. Pruning these layers is less aggressive for the model than pruning the attention layers.

I decided to prune neurons rather than entire layers because it results in a smaller alteration of the structure and avoids losing complete connections between layers. This way, the model retains its learning capacity more effectively.

Notebook: [12_pruning_structured.ipynb](https://github.com/peremartra/LLMOptCost/blob/main/11/12_pruning_structured.ipynb)

## Knowledge distillation. 
After completing the pruning process, I applied Knowledge Distillation to the pruned model. Since the goal was not to train the model for a specific task but rather to have it mimic the responses of the base model, selecting the right training datasets has been a hard decision.

Generalist Dataset: [ag_news](https://huggingface.co/datasets/fancyzhx/ag_news)

* The first dataset is a general news dataset with around 120K records classified into five categories. This dataset was used to ensure the model could mimic the base model’s responses across a wide range of topics, while retaining general knowledge and versatility.

Custom Dataset: [knowledge_transfer_1500_base](knowledge_transfer_1500_base )

I created the second dataset specifically to help the model generate short responses. It contains 1,200 short phrases across 10 topics, with each phrase being under 50 characters in length, most between 15 and 30 characters.

Using this custom dataset, I trained a new model based on the base model, and then performed a second round of Knowledge Distillation on the model that was trained using the AG News dataset. This two-step process allowed the model to first retain general knowledge and then specialize in short, concise responses.

Neither of the two datasets contains predefined answers, so the training is entirely based on the responses generated by the model. This approach allows the student model to learn directly from the teacher model's outputs, facilitating that the student model mimics the teacher’s behavior as closely as possible.

Notebook: [13_KnowledgeDistillationMeta.ipynb](https://github.com/peremartra/LLMOptCost/blob/main/11/13_KnowledgeDistillationMeta.ipynb)

### Models: 
I trained the models over 30 epochs in a A100 GPU on Google Colab. 

* [bloomz-560m-pruned-kdi-agnews](https://huggingface.co/oopere/bloomz-560m-pruned-kdi-agnews)
  
Epoch 1/30, Loss: 18.843030220667522 - Epoch 30/30, Loss: 0.9945140814781189

bloomz-560m-pruned trained with ag_news dataset. 

* [bloomz-560m-pruned-kdi-kt1](https://huggingface.co/oopere/bloomz-560m-pruned-kdi-kt1)
  
Epoch 1/30, Loss: 2.7381242191266715 - Epoch 30/30, Loss: 0.07271303632443056

bloomz-560m-pruned trained with knowledge_transfer_1500_base dataset. 

* [bloomz-560m-pruned-kdi-both](https://huggingface.co/oopere/bloomz-560m-pruned-kdi-both)
  
Epoch 1/30, Loss: 1.280850833129583 - Epoch 30/30, Loss: 0.0665320488360693

bloomz-560m-pruned-kdi-agnews trained with knowledge_transfer_1500_base dataset.

* [bloomz-560m-pruned-kdi-bothb](https://huggingface.co/oopere/bloomz-560m-pruned-kdi-bothb)
  
Epoch 1/30, Loss: 12.336102263132732 - Epoch 30/30, Loss: 1.114174348115921

bloomz-560m-pruned-kdi-kti trained with ag_news dataset.

## Testing. 
I created 2 different interfaces with Gradio. 

[11-BatchGradioInterface.ipynb](https://github.com/peremartra/LLMOptCost/blob/main/11/11-BatchGradioInterface.ipynb)
You can load an example .csv file containing a list of prompts, and it will execute them on three different models, displaying the average response time for each model and the cosine similarity of the generated responses.
<img src="https://github.com/peremartra/LLMOptCost/blob/main/img/gradio_batch_o.png">

In addition to calculating the average, a dataset is created for each of the prompts used, including the generated responses from the three models, the average response time, and the cosine similarity difference compared to the base model's generation.
<img src="https://github.com/peremartra/LLMOptCost/blob/main/img/df_results.png">

Another way to measure the quality of the models would have been to use LangSmith evaluators. In LangSmith, you can create datasets with the results from the LLMs and custom evaluators to measure different metrics. However, I decided not to use this tool due to the additional time required for setup and to simplify having both the code and results in a single, easy-to-share location. Anyway, if you want to check a example of how to use langsmith to measure the quality in LLMs check this notebook: [4_2_Evaluating_summaries_embeddings.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/4-Evaluating%20LLMs/4_2_Evaluating_summaries_embeddings.ipynb)

[11_GradioInterface.ipynb](https://github.com/peremartra/LLMOptCost/blob/main/11/11_GradioInterface.ipynb)
This interface is prepared to work with one model an compare the results with the pruned and the base model. Yo can introduce diferents prompts and get the results for every individual prompt. 
<img src="https://github.com/peremartra/LLMOptCost/blob/main/img/gradio_ind.png">

An interesting feature is that you can use BitsAndBytes library from Hugging Face to quantize the models tested. There are two different configurations preconfigured, one to save more memory (4 bits quantization) and other that save memory but produce better results in inference (8 bits quantization). 

I recommend using, at most, 8-bit quantization, completely ruling out 4-bit quantization for a model of this size. The memory savings are significant, but there is a noticeable loss in result accuracy when using 4-bit quantization, especially when applied post-training, as it cannot be corrected.
<img src="https://github.com/peremartra/LLMOptCost/blob/main/img/quantization.png">
*This image shows the original and unquantized values of a cosine. First, the values are quantized to 8 and 4 bits, then they are dequantized, and the result of the dequantization is plotted.*

# Conclusions. 

It is difficult to consider these conclusions as definitive. The small size of the test dataset and the use of a very reduced model prevent drawing highly conclusive insights. However, the results are promising, and when studied alongside existing documentation, it can be suggested that the use of pruning combined with knowledge distillation (KD) can yield very good results in creating more efficient and equally effective models.

## Results:
**bloomz-560m-pruned-kdi-both:**
*   Base Model Time: 0.2703
*   Model Time: 0.0978
*   Pruned Cosine Distance to Base: 0.1790
*   Model Cosine Distance to Base: 0.1838

**bloomz-560m-pruned-kdi-bothb:**
*   Base Model Time: 0.3221
*   Model Time: 0.1505
*   Pruned Cosine Distance to Base: 0.1790
*   Model Cosine Distance to Base: 0.1925

**bloomz-560m-pruned-kdi-agnews:**
*   Base Model Time: 0.3416
*   Model Time: 0.1214
*   Pruned Cosine Distance to Base: 0.1790
*   Model Cosine Distance to Base: 0.1639

**bloomz-560m-pruned-kdi-kt1:**
*   Base Model Time: 0.3568
*   Model Time: 0.1738
*   Pruned Cosine Distance to Base: 0.1790
*   Model Cosine Distance to Base: 0.2181

As seen in the results, the models that went through the pruning process are much more efficient than the base model, with an average response time reduction consistently above 60%.

Regarding the similarity of the responses to the base model, only one of the pruned models was able to produce responses that were closer to the base model's outputs. Notably, this was the only model that wasn't trained with the short-phrases dataset, suggesting that its selection may not have been the most fortunate choice.

However, it is important to note that these results were obtained using a test dataset of only 30 manually created prompts, so the conclusions should be taken merely as an indication.

The conclusion that the KD process helps the model recover its functionalities is also mentioned in other studies.'We observe qualitative agreement between both metrics. Curiously,
the results show different trends for pruning and quantization. For pruning, the representations tend
to become closer to the original representation during fine-tuning' [(Kuzmin et al., 2023)](https://arxiv.org/abs/2307.02973)

Quantization has taken a backseat in this context, as its main advantage lies in the memory savings it offers, though it often comes with a drop in processing performance. In any case, one of the notebooks includes tests to evaluate the impact of post-training quantization, meaning it was applied after the KD process.

## Improvements. 
* Datasets. When training the model to mimic its base model in any response, it is essential to use highly generalist datasets. A second dataset similar to AGNews could be a good option to explore.
* Model. The model used was of a very limited size, which likely impacted its ability to learn during the KD process. Although the loss obtained in each epoch showed that the model could still continue learning over more epochs, perhaps the only thing missing was additional processing time. In any case, it would be very interesting to see how the proposed solution performs with larger models.
* Prunig. Structured pruning should be tested, where the importance of the weights to be removed is calculated beforehand. Additionally, modern GPUs handle this type of pruning more effectively, so the efficiency gain could even surpass the current results.
Different levels of pruning could be applied to different layers, as opposed to the current solution where all layers underwent a uniform 30% pruning.

## Limitations

It has been a task with many resource limitations, both in terms of time, access to information, and processing power. Despite this, it has successfully validated a recently published idea: the use of KD combined with pruning can produce models that are both efficient and effective.

The use of shared GPUs in a cloud environment has limited the metrics available for measuring performance. I relied solely on execution time, as all models were running simultaneously on the same GPU.

The chosen metric for measuring text quality was cosine similarity of the embeddings. While I could have used a metric like ROUGE, I believe it would have contributed very little in this context.









