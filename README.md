# LLMOptCost

# Final Solution. 
I created a model with pruning and a double Knowledge Distillation Process. There is also the possibility to Apply quantization (but not mandatory) to the model, but after training, just to avoid memory problems when loading the Model into the GPU. 

With this solution, I have tried to replicate, within the limits, the work done by Nvidia with Llama models [arXiv:2407.14679 [cs.CL]](https://arxiv.org/abs/2407.14679), where they applied Pruning and Knowledge Distillation to create more efficient yet equally effective Llama models.

## Pruning
The pruning method I used is realy simple: L1 unstructured pruning, where 30% of the weights in each linear layer are set to zero based on their magnitude. This method is highly simple and flexible, allowing it to be applied to any model without the need for complex pre-computations.

In contrast, NVIDIA employs a much more advanced pruning strategy that first calculates the importance of different layers, neurons, and attention heads using a sample dataset. This importance ranking allows them to prune more effectively, maintaining higher accuracy while optimizing the model for structured sparsity, which is particularly efficient on modern GPUs.

While NVIDIA’s method is undoubtedly more optimal for performance and hardware efficiency, especially in large-scale deployments, my approach is simple and reusable, as it can be applied across various models without significant modification.

Notebook: [12_pruning.ipynb](https://github.com/peremartra/LLMOptCost/blob/main/11/12_pruning.ipynb)

Model Created: [bloomz-560m-pruned](https://huggingface.co/oopere/bloomz-560m-pruned)

## Knowledge distillation. 
After completing the pruning process, I applied Knowledge Distillation to the pruned model. Since the goal was not to train the model for a specific task but rather to have it mimic the responses of the base model, selecting the right training datasets has been a hard decision.

Generalist Dataset: [ag_news](https://huggingface.co/datasets/fancyzhx/ag_news)

* The first dataset is a general news dataset with around 120K records classified into five categories. This dataset was used to ensure the model could mimic the base model’s responses across a wide range of topics, while retaining general knowledge and versatility.

Custom Dataset: [knowledge_transfer_1500_base](knowledge_transfer_1500_base )

I created the second dataset specifically to help the model generate short responses. It contains 1,200 short phrases across 10 topics, with each phrase being under 50 characters in length, most between 15 and 30 characters.

Using this custom dataset, I trained a new model based on the base model, and then performed a second round of Knowledge Distillation on the model that was trained using the AG News dataset. This two-step process allowed the model to first retain general knowledge and then specialize in short, concise responses.

Neither of the two datasets contains predefined answers, so the training is entirely based on the responses generated by the model. This approach allows the student model to learn directly from the teacher model's outputs, facilitating that the student model mimics the teacher’s behavior as closely as possible.

### Models: 
I trained the models over 30 epochs in a A100 GPU on Google Colab. 

* "bloomz-560m-pruned-kdi-agnews"
Epoch 1/30, Loss: 18.843030220667522 - Epoch 30/30, Loss: 0.9945140814781189
bloomz-560m-pruned trained with ag_news dataset. 

* "bloomz-560m-pruned-kdi-kt1"
Epoch 1/30, Loss: 2.7381242191266715 - Epoch 30/30, Loss: 0.07271303632443056
bloomz-560m-pruned trained with knowledge_transfer_1500_base dataset. 

* "bloomz-560m-pruned-kdi-both"
Epoch 1/30, Loss: 1.280850833129583 - Epoch 30/30, Loss: 0.0665320488360693
bloomz-560m-pruned-kdi-agnews trained with knowledge_transfer_1500_base dataset.

## Testing. 



